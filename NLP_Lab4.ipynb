{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['au',\n",
       " 'aux',\n",
       " 'avec',\n",
       " 'ce',\n",
       " 'ces',\n",
       " 'dans',\n",
       " 'de',\n",
       " 'des',\n",
       " 'du',\n",
       " 'elle',\n",
       " 'en',\n",
       " 'et',\n",
       " 'eux',\n",
       " 'il',\n",
       " 'ils',\n",
       " 'je',\n",
       " 'la',\n",
       " 'le',\n",
       " 'les',\n",
       " 'leur',\n",
       " 'lui',\n",
       " 'ma',\n",
       " 'mais',\n",
       " 'me',\n",
       " 'même',\n",
       " 'mes',\n",
       " 'moi',\n",
       " 'mon',\n",
       " 'ne',\n",
       " 'nos',\n",
       " 'notre',\n",
       " 'nous',\n",
       " 'on',\n",
       " 'ou',\n",
       " 'par',\n",
       " 'pas',\n",
       " 'pour',\n",
       " 'qu',\n",
       " 'que',\n",
       " 'qui',\n",
       " 'sa',\n",
       " 'se',\n",
       " 'ses',\n",
       " 'son',\n",
       " 'sur',\n",
       " 'ta',\n",
       " 'te',\n",
       " 'tes',\n",
       " 'toi',\n",
       " 'ton',\n",
       " 'tu',\n",
       " 'un',\n",
       " 'une',\n",
       " 'vos',\n",
       " 'votre',\n",
       " 'vous',\n",
       " 'c',\n",
       " 'd',\n",
       " 'j',\n",
       " 'l',\n",
       " 'à',\n",
       " 'm',\n",
       " 'n',\n",
       " 's',\n",
       " 't',\n",
       " 'y',\n",
       " 'été',\n",
       " 'étée',\n",
       " 'étées',\n",
       " 'étés',\n",
       " 'étant',\n",
       " 'étante',\n",
       " 'étants',\n",
       " 'étantes',\n",
       " 'suis',\n",
       " 'es',\n",
       " 'est',\n",
       " 'sommes',\n",
       " 'êtes',\n",
       " 'sont',\n",
       " 'serai',\n",
       " 'seras',\n",
       " 'sera',\n",
       " 'serons',\n",
       " 'serez',\n",
       " 'seront',\n",
       " 'serais',\n",
       " 'serait',\n",
       " 'serions',\n",
       " 'seriez',\n",
       " 'seraient',\n",
       " 'étais',\n",
       " 'était',\n",
       " 'étions',\n",
       " 'étiez',\n",
       " 'étaient',\n",
       " 'fus',\n",
       " 'fut',\n",
       " 'fûmes',\n",
       " 'fûtes',\n",
       " 'furent',\n",
       " 'sois',\n",
       " 'soit',\n",
       " 'soyons',\n",
       " 'soyez',\n",
       " 'soient',\n",
       " 'fusse',\n",
       " 'fusses',\n",
       " 'fût',\n",
       " 'fussions',\n",
       " 'fussiez',\n",
       " 'fussent',\n",
       " 'ayant',\n",
       " 'ayante',\n",
       " 'ayantes',\n",
       " 'ayants',\n",
       " 'eu',\n",
       " 'eue',\n",
       " 'eues',\n",
       " 'eus',\n",
       " 'ai',\n",
       " 'as',\n",
       " 'avons',\n",
       " 'avez',\n",
       " 'ont',\n",
       " 'aurai',\n",
       " 'auras',\n",
       " 'aura',\n",
       " 'aurons',\n",
       " 'aurez',\n",
       " 'auront',\n",
       " 'aurais',\n",
       " 'aurait',\n",
       " 'aurions',\n",
       " 'auriez',\n",
       " 'auraient',\n",
       " 'avais',\n",
       " 'avait',\n",
       " 'avions',\n",
       " 'aviez',\n",
       " 'avaient',\n",
       " 'eut',\n",
       " 'eûmes',\n",
       " 'eûtes',\n",
       " 'eurent',\n",
       " 'aie',\n",
       " 'aies',\n",
       " 'ait',\n",
       " 'ayons',\n",
       " 'ayez',\n",
       " 'aient',\n",
       " 'eusse',\n",
       " 'eusses',\n",
       " 'eût',\n",
       " 'eussions',\n",
       " 'eussiez',\n",
       " 'eussent']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('french')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133737"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "entries = nltk.corpus.cmudict.entries()  #CMU Colllege\n",
    "len(entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('bye.n.01'), Synset('adieu.n.01')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "wn.synsets(\"bye\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bye', 'pass']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('bye.n.01').lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adieu',\n",
       " 'adios',\n",
       " 'arrivederci',\n",
       " 'auf_wiedersehen',\n",
       " 'au_revoir',\n",
       " 'bye',\n",
       " 'bye-bye',\n",
       " 'cheerio',\n",
       " 'good-by',\n",
       " 'goodby',\n",
       " 'good-bye',\n",
       " 'goodbye',\n",
       " 'good_day',\n",
       " 'sayonara',\n",
       " 'so_long']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('adieu.n.01').lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', ['AH0'])\n",
      "('a.', ['EY1'])\n",
      "('a', ['EY1'])\n",
      "('a42128', ['EY1', 'F', 'AO1', 'R', 'T', 'UW1', 'W', 'AH1', 'N', 'T', 'UW1', 'EY1', 'T'])\n",
      "('aaa', ['T', 'R', 'IH2', 'P', 'AH0', 'L', 'EY1'])\n",
      "('aaberg', ['AA1', 'B', 'ER0', 'G'])\n",
      "('aachen', ['AA1', 'K', 'AH0', 'N'])\n",
      "('aachener', ['AA1', 'K', 'AH0', 'N', 'ER0'])\n",
      "('aaker', ['AA1', 'K', 'ER0'])\n",
      "('aalseth', ['AA1', 'L', 'S', 'EH0', 'TH'])\n",
      "('aamodt', ['AA1', 'M', 'AH0', 'T'])\n",
      "('aancor', ['AA1', 'N', 'K', 'AO2', 'R'])\n",
      "('aardema', ['AA0', 'R', 'D', 'EH1', 'M', 'AH0'])\n",
      "('aardvark', ['AA1', 'R', 'D', 'V', 'AA2', 'R', 'K'])\n",
      "('aaron', ['EH1', 'R', 'AH0', 'N'])\n",
      "(\"aaron's\", ['EH1', 'R', 'AH0', 'N', 'Z'])\n",
      "('aarons', ['EH1', 'R', 'AH0', 'N', 'Z'])\n",
      "('aaronson', ['EH1', 'R', 'AH0', 'N', 'S', 'AH0', 'N'])\n",
      "('aaronson', ['AA1', 'R', 'AH0', 'N', 'S', 'AH0', 'N'])\n",
      "(\"aaronson's\", ['EH1', 'R', 'AH0', 'N', 'S', 'AH0', 'N', 'Z'])\n",
      "(\"aaronson's\", ['AA1', 'R', 'AH0', 'N', 'S', 'AH0', 'N', 'Z'])\n",
      "('aarti', ['AA1', 'R', 'T', 'IY2'])\n",
      "('aase', ['AA1', 'S'])\n",
      "('aasen', ['AA1', 'S', 'AH0', 'N'])\n",
      "('ab', ['AE1', 'B'])\n",
      "('ab', ['EY1', 'B', 'IY1'])\n",
      "('ababa', ['AH0', 'B', 'AA1', 'B', 'AH0'])\n",
      "('ababa', ['AA1', 'B', 'AH0', 'B', 'AH0'])\n",
      "('abacha', ['AE1', 'B', 'AH0', 'K', 'AH0'])\n",
      "('aback', ['AH0', 'B', 'AE1', 'K'])\n",
      "('abaco', ['AE1', 'B', 'AH0', 'K', 'OW2'])\n",
      "('abacus', ['AE1', 'B', 'AH0', 'K', 'AH0', 'S'])\n",
      "('abad', ['AH0', 'B', 'AA1', 'D'])\n",
      "('abadaka', ['AH0', 'B', 'AE1', 'D', 'AH0', 'K', 'AH0'])\n",
      "('abadi', ['AH0', 'B', 'AE1', 'D', 'IY0'])\n",
      "('abadie', ['AH0', 'B', 'AE1', 'D', 'IY0'])\n",
      "('abair', ['AH0', 'B', 'EH1', 'R'])\n",
      "('abalkin', ['AH0', 'B', 'AA1', 'L', 'K', 'IH0', 'N'])\n",
      "('abalone', ['AE2', 'B', 'AH0', 'L', 'OW1', 'N', 'IY0'])\n",
      "('abalos', ['AA0', 'B', 'AA1', 'L', 'OW0', 'Z'])\n",
      "('abandon', ['AH0', 'B', 'AE1', 'N', 'D', 'AH0', 'N'])\n",
      "('abandoned', ['AH0', 'B', 'AE1', 'N', 'D', 'AH0', 'N', 'D'])\n",
      "('abandoning', ['AH0', 'B', 'AE1', 'N', 'D', 'AH0', 'N', 'IH0', 'NG'])\n",
      "('abandonment', ['AH0', 'B', 'AE1', 'N', 'D', 'AH0', 'N', 'M', 'AH0', 'N', 'T'])\n",
      "('abandonments', ['AH0', 'B', 'AE1', 'N', 'D', 'AH0', 'N', 'M', 'AH0', 'N', 'T', 'S'])\n",
      "('abandons', ['AH0', 'B', 'AE1', 'N', 'D', 'AH0', 'N', 'Z'])\n",
      "('abanto', ['AH0', 'B', 'AE1', 'N', 'T', 'OW0'])\n",
      "('abarca', ['AH0', 'B', 'AA1', 'R', 'K', 'AH0'])\n",
      "('abare', ['AA0', 'B', 'AA1', 'R', 'IY0'])\n",
      "('abascal', ['AE1', 'B', 'AH0', 'S', 'K', 'AH0', 'L'])\n"
     ]
    }
   ],
   "source": [
    "for entry in entries[:50]:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\PURUSHOTTAM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenization into sentences and words\n",
    "\n",
    "lines = ['''Jacob Benjamin Gyllenhaal (/ˈdʒɪlənhɔːl/;[1] born December 19, 1980) is an American actor and film producer. Born into the Gyllenhaal family, he is the son of director Stephen Gyllenhaal and screenwriter Naomi Foner. He began acting as a child, making his acting debut in City Slickers (1991), followed by roles in his father's films A Dangerous Woman (1993) and Homegrown (1998). His breakthrough performances were as Homer Hickam in October Sky (1999) and as a psychologically troubled teenager in Donnie Darko (2001). His most widely seen film to that point came with the disaster film The Day After Tomorrow (2004).\n",
    "\n",
    "Gyllenhaal won the BAFTA Award for Best Actor in a Supporting Role and received a nomination for the Academy Award for Best Supporting Actor for playing Jack Twist in Ang Lee's romance Brokeback Mountain (2005).''']\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Jacob', 'NNP'), ('Benjamin', 'NNP'), ('Gyllenhaal', 'NNP'), ('(', '('), ('/ˈdʒɪlənhɔːl/', 'NNP'), (';', ':'), ('[', 'VBZ'), ('1', 'CD'), (']', 'NN'), ('born', 'VBN'), ('December', 'NNP'), ('19', 'CD'), (',', ','), ('1980', 'CD'), (')', ')'), ('is', 'VBZ'), ('an', 'DT'), ('American', 'JJ'), ('actor', 'NN'), ('and', 'CC'), ('film', 'NN'), ('producer', 'NN'), ('.', '.')]\n",
      "[('Born', 'NNP'), ('into', 'IN'), ('the', 'DT'), ('Gyllenhaal', 'NNP'), ('family', 'NN'), (',', ','), ('he', 'PRP'), ('is', 'VBZ'), ('the', 'DT'), ('son', 'NN'), ('of', 'IN'), ('director', 'NN'), ('Stephen', 'NNP'), ('Gyllenhaal', 'NNP'), ('and', 'CC'), ('screenwriter', 'NN'), ('Naomi', 'NNP'), ('Foner', 'NNP'), ('.', '.')]\n",
      "[('He', 'PRP'), ('began', 'VBD'), ('acting', 'VBG'), ('as', 'IN'), ('a', 'DT'), ('child', 'NN'), (',', ','), ('making', 'VBG'), ('his', 'PRP$'), ('acting', 'VBG'), ('debut', 'NN'), ('in', 'IN'), ('City', 'NNP'), ('Slickers', 'NNP'), ('(', '('), ('1991', 'CD'), (')', ')'), (',', ','), ('followed', 'VBN'), ('by', 'IN'), ('roles', 'NNS'), ('in', 'IN'), ('his', 'PRP$'), ('father', 'NN'), (\"'s\", 'POS'), ('films', 'NNS'), ('A', 'NNP'), ('Dangerous', 'NNP'), ('Woman', 'NNP'), ('(', '('), ('1993', 'CD'), (')', ')'), ('and', 'CC'), ('Homegrown', 'NNP'), ('(', '('), ('1998', 'CD'), (')', ')'), ('.', '.')]\n",
      "[('His', 'PRP$'), ('breakthrough', 'NN'), ('performances', 'NNS'), ('were', 'VBD'), ('as', 'IN'), ('Homer', 'NNP'), ('Hickam', 'NNP'), ('in', 'IN'), ('October', 'NNP'), ('Sky', 'NNP'), ('(', '('), ('1999', 'CD'), (')', ')'), ('and', 'CC'), ('as', 'IN'), ('a', 'DT'), ('psychologically', 'RB'), ('troubled', 'JJ'), ('teenager', 'NN'), ('in', 'IN'), ('Donnie', 'NNP'), ('Darko', 'NNP'), ('(', '('), ('2001', 'CD'), (')', ')'), ('.', '.')]\n",
      "[('His', 'PRP$'), ('most', 'RBS'), ('widely', 'RB'), ('seen', 'VBN'), ('film', 'NN'), ('to', 'TO'), ('that', 'DT'), ('point', 'NN'), ('came', 'VBD'), ('with', 'IN'), ('the', 'DT'), ('disaster', 'NN'), ('film', 'NN'), ('The', 'DT'), ('Day', 'NNP'), ('After', 'IN'), ('Tomorrow', 'NNP'), ('(', '('), ('2004', 'CD'), (')', ')'), ('.', '.')]\n",
      "[('Gyllenhaal', 'NNP'), ('won', 'VBD'), ('the', 'DT'), ('BAFTA', 'NNP'), ('Award', 'NNP'), ('for', 'IN'), ('Best', 'NNP'), ('Actor', 'NNP'), ('in', 'IN'), ('a', 'DT'), ('Supporting', 'NNP'), ('Role', 'NNP'), ('and', 'CC'), ('received', 'VBD'), ('a', 'DT'), ('nomination', 'NN'), ('for', 'IN'), ('the', 'DT'), ('Academy', 'NNP'), ('Award', 'NNP'), ('for', 'IN'), ('Best', 'NNP'), ('Supporting', 'NNP'), ('Actor', 'NNP'), ('for', 'IN'), ('playing', 'VBG'), ('Jack', 'NNP'), ('Twist', 'NNP'), ('in', 'IN'), ('Ang', 'NNP'), ('Lee', 'NNP'), (\"'s\", 'POS'), ('romance', 'NN'), ('Brokeback', 'NNP'), ('Mountain', 'NNP'), ('(', '('), ('2005', 'CD'), (')', ')'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "for text in lines:\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    for sentence in sentences:\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        tagged_words = nltk.pos_tag(words)\n",
    "        print(tagged_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Twitter aware Tokenizer \n",
    "\n",
    "import nltk \n",
    "from nltk.tokenize import TweetTokenizer \n",
    "text = '''Democrats are now the party of high taxes, high crime, open borders, late-term abortion, socialism, and blatant corruption. The Republican Party is the party of the American Worker, the American Family, and the American Dream! #KAG2020'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "twtkn = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Democrats',\n",
       " 'are',\n",
       " 'now',\n",
       " 'the',\n",
       " 'party',\n",
       " 'of',\n",
       " 'high',\n",
       " 'taxes',\n",
       " ',',\n",
       " 'high',\n",
       " 'crime',\n",
       " ',',\n",
       " 'open',\n",
       " 'borders',\n",
       " ',',\n",
       " 'late-term',\n",
       " 'abortion',\n",
       " ',',\n",
       " 'socialism',\n",
       " ',',\n",
       " 'and',\n",
       " 'blatant',\n",
       " 'corruption',\n",
       " '.',\n",
       " 'The',\n",
       " 'Republican',\n",
       " 'Party',\n",
       " 'is',\n",
       " 'the',\n",
       " 'party',\n",
       " 'of',\n",
       " 'the',\n",
       " 'American',\n",
       " 'Worker',\n",
       " ',',\n",
       " 'the',\n",
       " 'American',\n",
       " 'Family',\n",
       " ',',\n",
       " 'and',\n",
       " 'the',\n",
       " 'American',\n",
       " 'Dream',\n",
       " '!',\n",
       " '#KAG2020']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #KAG2020 is recognized as a token\n",
    "twtkn.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
